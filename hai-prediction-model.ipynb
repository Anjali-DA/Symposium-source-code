{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1X3dFDE2bCHBn4RdVvxkb6WYJzbChWruU","authorship_tag":"ABX9TyNTn5aYuYKPZXTFWLCy+cUI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"nOuORTvlhwwp","executionInfo":{"status":"ok","timestamp":1684623621940,"user_tz":-330,"elapsed":3095,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import string\n","# Standardization \n","from sklearn.preprocessing import StandardScaler\n","import nltk\n","# remove stopwords\n","from nltk.corpus import stopwords\n","# lemmatization & pos tagging\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from datetime import datetime\n"]},{"cell_type":"code","source":["df= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/HAI dataset.csv')"],"metadata":{"id":"V9adov8siU0y","executionInfo":{"status":"ok","timestamp":1684623621943,"user_tz":-330,"elapsed":36,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["df.dropna()\n","df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSdEwd85uFIY","executionInfo":{"status":"ok","timestamp":1684623621944,"user_tz":-330,"elapsed":37,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"601f10df-1ce8-4409-bda0-9c9eda767eca"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 199 entries, 0 to 198\n","Data columns (total 17 columns):\n"," #   Column                Non-Null Count  Dtype \n","---  ------                --------------  ----- \n"," 0   Patient ID            199 non-null    int64 \n"," 1   Age                   199 non-null    int64 \n"," 2   Sex                   199 non-null    object\n"," 3   Race                  199 non-null    object\n"," 4   Ethnicity             199 non-null    object\n"," 5   Medical History       199 non-null    object\n"," 6   Hospitalization Data  199 non-null    object\n"," 7   Laboratory Data       199 non-null    object\n"," 8   Imaging Data          199 non-null    object\n"," 9   Microbiology Data     199 non-null    object\n"," 10  Risk Factors          199 non-null    object\n"," 11  Symptoms              199 non-null    object\n"," 12  Signs                 198 non-null    object\n"," 13  Treatment             198 non-null    object\n"," 14  Outcomes              198 non-null    object\n"," 15  HAI Name              198 non-null    object\n"," 16  Unnamed: 16           15 non-null     object\n","dtypes: int64(2), object(15)\n","memory usage: 26.6+ KB\n"]}]},{"cell_type":"markdown","source":["#finding Y"],"metadata":{"id":"FsZIGR-JrDGc"}},{"cell_type":"code","source":["df['HAI Name'].unique()\n","df['HAI Name']= df['HAI Name'].replace(['Migraine', 'Allergic Rhinitis',\n","       'NAFLD (Non-Alcoholic Fatty Liver Disease)', 'Type 2 Diabetes',\n","       'Osteoporotic Fracture', 'Hypertensive Heart Disease', 'Asthma',\n","       'Cardiovascular Disease', 'Hypothyroidism',\n","       'Breast Cancer', 'Allergic Dermatitis', 'Hypertensive Diabetes',\n","       'Diabetic Hypertension', 'Fracture', 'Hypertensive Kidney Disease',\n","     'Diabetic Infection', 'Bone Infection',\n","       'Cardiovascular Infection', 'Hypertensive Infection','Myocardial Infarction', 'Osteoarthritis', 'COPD', 'Hypertension',\n","       'Fatty Liver Disease', 'Coronary Artery Disease',\n","       'Asthma exacerbation','Hypertension, Non-alcoholic fatty liver disease', 'Heart failure',\n","       'Hypertensive cerebrovascular disease'],'other')\n","\n","df['HAI Name']= df['HAI Name'].map({'other':0,'Pneumonia':1,'Influenza':2,'Unspecified Infection':3,'COVID-19':4,'Respiratory Infection':5})\n","df['HAI Name'] = df['HAI Name'].replace('None', -1)\n","df['HAI Name'] = df['HAI Name'].fillna(-1)\n","df['HAI Name'].dropna()\n","print(len(df['HAI Name']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zNuYJk_Uq1HB","executionInfo":{"status":"ok","timestamp":1684623621945,"user_tz":-330,"elapsed":34,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"bb2d90bf-e09d-41b4-840c-6fabb7b1eea2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["199\n"]}]},{"cell_type":"markdown","source":["#data preprocessing"],"metadata":{"id":"JoSXt__-iKpg"}},{"cell_type":"code","source":["class Processing:\n","    def __init__(self, df):\n","        df1 = df.drop(['Patient ID', 'Unnamed: 16'], axis=1)\n","        df2 = df1.drop(['Race', 'Ethnicity', 'Outcomes'], axis=1)\n","        for index, row in df2.iterrows():\n","            try:\n","                start_date_str, end_date_str = row['Hospitalization Data'].split(' to ')\n","                start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n","                end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n","                days = (end_date - start_date).days\n","                df2.at[index, 'Hospitalization Data'] = days\n","            except ValueError:\n","                print(f\"Invalid date range at index {index}: {row['Hospitalization Data']}\")\n","        \n","        df2['Sex'] = df2['Sex'].replace(['Female'], 'F')\n","        df2['Sex'] = df2['Sex'].replace(['Male'], 'M')\n","        df2['Sex'] = df2['Sex'].map({'F': 1, 'M': 0}).astype(int)\n","        df2 = df2.replace('None', -1)\n","    \n","        nc = df2.select_dtypes(include=['int', 'float']).columns\n","        tc = df2.select_dtypes(include='object').columns\n","        nd = df2[nc]\n","        td = df2[tc]\n","        scaler = StandardScaler()\n","        std_nd = scaler.fit_transform(nd)\n","        std_nd = pd.DataFrame(std_nd, columns=nd.columns)\n","        \n","        # Text preprocessing\n","        # 1. Lowercase\n","        # for column in td.columns:\n","        #     if df2[column].dtype in [np.int64, np.float64]:\n","        #         continue\n","        #     else:\n","        #         td[column] = td[column].str.lower()\n","        \n","        # 2. Remove punctuation\n","        for column in td.columns:\n","            td[column] = td[column].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)) if isinstance(x, str) else x)\n","        \n","        # 3. Remove stopwords\n","        for column in td.columns:\n","            stop_words = set(stopwords.words('english'))\n","            td[column] = td[column].apply(lambda x: ' '.join([word for word in str(x).split() if word.lower() not in stop_words]))\n","        \n","        # 4. Lemmatizer\n","        lemmatizer = WordNetLemmatizer()\n","        \n","        def lemmatize_word(word, pos_tag):\n","            if pos_tag.startswith('J'):\n","                pos = wordnet.ADJ\n","            elif pos_tag.startswith('V'):\n","                pos = wordnet.VERB\n","            elif pos_tag.startswith('N'):\n","                pos = wordnet.NOUN\n","            elif pos_tag.startswith('R'):\n","                pos = wordnet.ADV\n","            else:\n","                pos = wordnet.NOUN\n","            return lemmatizer.lemmatize(word, pos)\n","\n","        def lemmatize_column(column):\n","            if column == 'NaN':\n","                return column\n","            elif isinstance(column, str):\n","                tokens = word_tokenize(column)\n","                pos_tags = nltk.pos_tag(tokens)\n","                lemmatized_words = [lemmatize_word(word, pos_tag) for word, pos_tag in pos_tags]\n","                return ' '.join(lemmatized_words)\n","            else:\n","                return column\n","        \n","        for column in td.columns:\n","            td.loc[:, column] = td[column].apply(lemmatize_column)\n","        \n","        # 5. Vectorizer\n","        text_columns = ['Medical History', 'Laboratory Data', 'Imaging Data', 'Microbiology Data',\n","                        'Risk Factors', 'Symptoms', 'Signs', 'Treatment']\n","\n","        td['Combined Text'] = td[text_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n","\n","        vectorizer = TfidfVectorizer()\n","        vectorized_data = vectorizer.fit_transform(td['Combined Text'])\n","\n","        self.vectorized_df = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n","        self.vectorized_text_data = pd.concat([td, self.vectorized_df], axis=1)\n","        \n","        # 6. Combine data\n","        self.combined_data = pd.concat([nd, self.vectorized_df], axis=1)\n","        self.combined_data = self.combined_data.fillna(-1)\n","        # self.combined_data = self.combined_data.dropna()\n","    \n","    def process_data(self):\n","        return self.combined_data\n"],"metadata":{"id":"e3Vuc0BShyVD","executionInfo":{"status":"ok","timestamp":1684623621946,"user_tz":-330,"elapsed":29,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# nltk.download('stopwords')\n","# nltk.download('punkt')\n","# nltk.download('averaged_perceptron_tagger')\n","# nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qe7hZ3XZkB59","executionInfo":{"status":"ok","timestamp":1684623634828,"user_tz":-330,"elapsed":1212,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"671f209c-d4a9-4c29-b53f-e7584a5526ce"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["processing = Processing(df)\n","combined_data = processing.process_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8oofyWqj1Ed","executionInfo":{"status":"ok","timestamp":1684623650515,"user_tz":-330,"elapsed":8933,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"a793a66b-86a5-4b40-fe3f-fb27bc1ecac5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-a205f18bb11c>:38: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td[column] = td[column].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)) if isinstance(x, str) else x)\n","<ipython-input-5-a205f18bb11c>:43: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td[column] = td[column].apply(lambda x: ' '.join([word for word in str(x).split() if word.lower() not in stop_words]))\n","<ipython-input-5-a205f18bb11c>:73: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td.loc[:, column] = td[column].apply(lemmatize_column)\n","<ipython-input-5-a205f18bb11c>:79: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td['Combined Text'] = td[text_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n"]}]},{"cell_type":"code","source":["combined_data.head()\n","# combined_data.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"TWH_oOxdimC2","executionInfo":{"status":"ok","timestamp":1684623661872,"user_tz":-330,"elapsed":12,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"5405ef0b-0220-4aa4-805d-1604c01eea61"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Age  Sex  Hospitalization Data  HAI Name  abdomen  abdominal  abnormal  \\\n","0   45    0                     7       1.0      0.0   0.000000  0.000000   \n","1   32    1                     8       2.0      0.0   0.000000  0.167199   \n","2   60    0                     7       0.0      0.0   0.000000  0.000000   \n","3   28    1                     6       0.0      0.0   0.000000  0.000000   \n","4   50    0                     8       0.0      0.0   0.272157  0.000000   \n","\n","   abnormality   alcohol  allergens  ...  urine  visual  wall  wbc  weak  \\\n","0     0.000000  0.000000        0.0  ...    0.0     0.0   0.0  0.0   0.0   \n","1     0.000000  0.000000        0.0  ...    0.0     0.0   0.0  0.0   0.0   \n","2     0.469022  0.000000        0.0  ...    0.0     0.0   0.0  0.0   0.0   \n","3     0.000000  0.000000        0.0  ...    0.0     0.0   0.0  0.0   0.0   \n","4     0.000000  0.314017        0.0  ...    0.0     0.0   0.0  0.0   0.0   \n","\n","   weakness  weight  wheezing     white      xray  \n","0       0.0     0.0       0.0  0.000000  0.163416  \n","1       0.0     0.0       0.0  0.000000  0.000000  \n","2       0.0     0.0       0.0  0.000000  0.176446  \n","3       0.0     0.0       0.0  0.349131  0.000000  \n","4       0.0     0.0       0.0  0.000000  0.000000  \n","\n","[5 rows x 226 columns]"],"text/html":["\n","  <div id=\"df-0dc09f1d-794f-4f15-9fd1-c15f13788fe4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Age</th>\n","      <th>Sex</th>\n","      <th>Hospitalization Data</th>\n","      <th>HAI Name</th>\n","      <th>abdomen</th>\n","      <th>abdominal</th>\n","      <th>abnormal</th>\n","      <th>abnormality</th>\n","      <th>alcohol</th>\n","      <th>allergens</th>\n","      <th>...</th>\n","      <th>urine</th>\n","      <th>visual</th>\n","      <th>wall</th>\n","      <th>wbc</th>\n","      <th>weak</th>\n","      <th>weakness</th>\n","      <th>weight</th>\n","      <th>wheezing</th>\n","      <th>white</th>\n","      <th>xray</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>45</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.163416</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.167199</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>60</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.469022</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.176446</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>28</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.349131</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>50</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.272157</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.314017</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 226 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0dc09f1d-794f-4f15-9fd1-c15f13788fe4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0dc09f1d-794f-4f15-9fd1-c15f13788fe4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0dc09f1d-794f-4f15-9fd1-c15f13788fe4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["#model training"],"metadata":{"id":"AELnT-P-m0GM"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import GridSearchCV"],"metadata":{"id":"-2sQ8GEmm2zh","executionInfo":{"status":"ok","timestamp":1684623679779,"user_tz":-330,"elapsed":648,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["combined_data.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rfXbOfOtDwE","executionInfo":{"status":"ok","timestamp":1684623683361,"user_tz":-330,"elapsed":5,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"46db7a53-4fce-4783-f329-75e03c0dc0af"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 199 entries, 0 to 198\n","Columns: 226 entries, Age to xray\n","dtypes: float64(223), int64(3)\n","memory usage: 351.5 KB\n"]}]},{"cell_type":"code","source":["df['HAI Name'].info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDLmyCm-tJFc","executionInfo":{"status":"ok","timestamp":1684623688586,"user_tz":-330,"elapsed":493,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"48758673-927d-4fb2-e562-1b0b8b588324"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.series.Series'>\n","RangeIndex: 199 entries, 0 to 198\n","Series name: HAI Name\n","Non-Null Count  Dtype  \n","--------------  -----  \n","199 non-null    float64\n","dtypes: float64(1)\n","memory usage: 1.7 KB\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"sQR8hHaKZXfg"}},{"cell_type":"code","source":["X = combined_data.drop('HAI Name', axis=1)\n","\n","y = combined_data['HAI Name']\n","X.info()\n","print(len(X))\n","print(len(y))\n","# len(X) == len(y)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wlBcGn1m3MP","executionInfo":{"status":"ok","timestamp":1684623827584,"user_tz":-330,"elapsed":1757,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"35a887d2-d689-4177-e7ef-920b58ee321b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 199 entries, 0 to 198\n","Columns: 225 entries, Age to xray\n","dtypes: float64(222), int64(3)\n","memory usage: 349.9 KB\n","199\n","199\n"]}]},{"cell_type":"code","source":["# # svm\n","# svm = SVC()\n","# svm.fit(X_train, y_train)\n","\n","# y_pred = svm.predict(X_test)\n","# svm_accuracy = accuracy_score(y_test, y_pred)*100\n","# print(\"Accuracy:\", svm_accuracy, \"%\")\n","\n","# svm_score = cross_val_score(svm, X_train, y_train, cv=6)\n","# print('svm cross validation score',round(svm_score.mean()*100,2).astype(str),'%' )\n","\n"],"metadata":{"id":"S1oQKpr3m7l9","executionInfo":{"status":"aborted","timestamp":1684623621952,"user_tz":-330,"elapsed":26,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random Forest Classifer\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train, y_train)\n","\n","y_pred = rf.predict(X_test)\n","rf_accuracy = accuracy_score(y_test, y_pred)*100\n","print(\"Accuracy:\", rf_accuracy, \"%\")\n","\n","rf_score = cross_val_score(rf, X_train, y_train, cv=6)\n","print('rfc cross validation score',round(rf_score.mean()*100,2).astype(str),'%' )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-ErnXdinFmf","executionInfo":{"status":"ok","timestamp":1684623844867,"user_tz":-330,"elapsed":1729,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"bc108c8b-8dd3-4d5b-cf56-89cc33eef3ee"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 87.5 %\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=6.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["rfc cross validation score 90.69 %\n"]}]},{"cell_type":"code","source":["#  Naive bayes\n","nb = GaussianNB()\n","nb.fit(X_train, y_train)\n","\n","y_pred = nb.predict(X_test)\n","nb_accuracy = accuracy_score(y_test, y_pred)*100\n","print(\"Accuracy:\", nb_accuracy, \"%\")\n","\n","nb_score= cross_val_score(nb,X_train,y_train,cv=6)\n","print('naive bayes cross validation score',round(nb_score.mean()*100,2).astype(str),'%' )"],"metadata":{"id":"0j1sprZsnF1s","executionInfo":{"status":"aborted","timestamp":1684623621953,"user_tz":-330,"elapsed":26,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GridSearchCV\n","svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n","grid_svc = GridSearchCV(SVC(), svc_params)\n","grid_svc.fit(X_train, y_train)\n","svc = grid_svc.best_estimator_\n","\n","y_pred = grid_svc.predict(X_test)\n","svc_accuracy = accuracy_score(y_test, y_pred)*100\n","print(\"Accuracy:\", nb_accuracy, \"%\")\n","\n","svc_score= cross_val_score(svc,X_train,y_train,cv=6)\n","print('svc cross validation score',round(svc_score.mean()*100,2).astype(str),'%' )"],"metadata":{"id":"W1Po5cmJnGGT","executionInfo":{"status":"aborted","timestamp":1684623621953,"user_tz":-330,"elapsed":26,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#output predicting"],"metadata":{"id":"Enb961F7xwsY"}},{"cell_type":"code","source":["# data_list = [1, 45, 'M', 'White', 'Non-Hispanic', 'Asthma, Hypertension', '2023-01-05 to 2023-01-12', 'Blood Test: Normal, X-ray: Clear', 'MRI: No abnormalities', 'NonNasal swab: Positive for influenzae', 'Obesity, Smoking', 'Cough, Fever', 'Elevated heart rate', 'Antibiotics', 'Recovered','None']\n","# columns = ['Patient ID','Age', 'Sex', 'Race', 'Ethnicity', 'Medical History', 'Hospitalization Data', 'Laboratory Data', 'Imaging Data', 'Microbiology Data', 'Risk Factors', 'Symptoms', 'Signs', 'Treatment', 'Outcomes','Unnamed: 16']\n","# df_new = pd.DataFrame([data_list], columns=columns)\n","# test_df = Processing(df_new)\n","# X_data = test_df.process_data()\n","# X_data.head()\n","\n","# # # Step 1: Get feature names used during training\n","# # feature_names_training = X_train.columns.tolist()\n","\n","# # # Step 2: Align feature names in test data\n","# # feature_names_test = X_data.columns.tolist()\n","# # missing_features = set(feature_names_training) - set(feature_names_test)\n","\n","# # # Step 3: Remove unmatched features from test data\n","# # for feature in missing_features:\n","# #     if feature in X_data.columns:\n","# #         X_data = X_data.drop(feature, axis=1)\n","\n","# # # Step 4: Fill missing features with zero\n","# # for feature in feature_names_training:\n","# #     if feature not in X_data.columns:\n","# #         X_data[feature] = 0\n","\n","# # WORKING CODE\n","# # # Step 1: Get feature names used during training\n","# # feature_names_training = X_train.columns.tolist()\n","\n","# # # Step 2: Align feature names in test data\n","# # feature_names_test = X_data.columns.tolist()\n","\n","# # # Step 3: Identify unseen features in test data\n","# # unseen_features = set(feature_names_test) - set(feature_names_training)\n","\n","# # # Step 4: Drop unseen features from test data\n","# # X_data = X_data.drop(unseen_features, axis=1)\n","\n","# # # Step 5: Add missing features to test data with zero values\n","# # missing_features = set(feature_names_training) - set(feature_names_test)\n","# # for feature in missing_features:\n","# #     X_data[feature] = 0\n","\n","# # # Step 6: Make predictions using the Random Forest classifier\n","# # predict = rf.predict(X_data)\n","# # print(predict)\n","\n","\n","# # Step 1: Get feature names used during training\n","# feature_names_training = X_train.columns.tolist()\n","\n","# # Step 2: Align feature names in test data\n","# feature_names_test = X_data.columns.tolist()\n","\n","# # Step 3: Identify unseen features in test data\n","# unseen_features = set(feature_names_test) - set(feature_names_training)\n","\n","# # Step 4: Drop unseen features from test data\n","# X_data = X_data.drop(unseen_features, axis=1)\n","\n","# # Step 5: Reorder features in test data to match the order in training data\n","# X_data = X_data[feature_names_training]\n","\n","# # Step 6: Add missing features to test data with zero values\n","# missing_features = set(feature_names_training) - set(feature_names_test)\n","# for feature in missing_features:\n","#     X_data[feature] = 0\n","\n","# # Step 7: Make predictions using the Random Forest classifier\n","# predict = rf.predict(X_data)\n","# print(predict)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":970},"id":"hUikfclEx0Pv","executionInfo":{"status":"error","timestamp":1684626607114,"user_tz":-330,"elapsed":1084,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"ab7dd51b-ca07-47db-8d94-7656db34990a"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-a205f18bb11c>:38: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td[column] = td[column].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)) if isinstance(x, str) else x)\n","<ipython-input-5-a205f18bb11c>:43: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td[column] = td[column].apply(lambda x: ' '.join([word for word in str(x).split() if word.lower() not in stop_words]))\n","<ipython-input-5-a205f18bb11c>:73: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td.loc[:, column] = td[column].apply(lemmatize_column)\n","<ipython-input-5-a205f18bb11c>:79: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td['Combined Text'] = td[text_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-f555f0adaf4a>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Step 5: Reorder features in test data to match the order in training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mX_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names_training\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Step 6: Add missing features to test data with zero values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3813\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3815\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6068\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6072\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6132\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6133\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6135\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['Hospitalization Data', 'abdomen', 'abdominal', 'abnormal', 'alcohol', 'allergens', 'allergies', 'angiogram', 'anklebrachial', 'antihistamines', 'antiviral', 'apnea', 'artery', 'arthritis', 'back', 'bilateral', 'blocked', 'bone', 'brain', 'breast', 'breath', 'breathe', 'breathing', 'bronchial', 'cancer', 'capacity', 'care', 'cell', 'cerebral', 'change', 'chemotherapy', 'chest', 'cholesterol', 'chronic', 'co2', 'congestion', 'consolidation', 'consumption', 'copd', 'coronary', 'coughing', 'count', 'cream', 'creatinine', 'ct', 'culture', 'decreased', 'deformity', 'degenerative', 'density', 'diabetes', 'dialysis', 'dietary', 'disc', 'discomfort', 'disease', 'disorder', 'disturbance', 'dizziness', 'dry', 'ecg', 'echocardiogram', 'eczema', 'ejection', 'enlarged', 'enlargement', 'enzyme', 'exposure', 'family', 'fatigue', 'fatty', 'flow', 'fraction', 'fracture', 'fractured', 'function', 'gain', 'gland', 'glucose', 'hai', 'hair', 'hba1c', 'headache', 'heartbeat', 'height', 'herniation', 'high', 'history', 'hormone', 'hydration', 'hyperlipidemia', 'hyperthyroidism', 'hypothyroidism', 'increased', 'index', 'infarct', 'infection', 'infiltrate', 'infiltrates', 'inflammation', 'influenza', 'inhaler', 'injury', 'insulin', 'irregular', 'itchy', 'jaundice', 'joint', 'kidney', 'kyphosis', 'leg', 'level', 'lifestyle', 'light', 'limited', 'liver', 'loss', 'low', 'lumbar', 'lump', 'lung', 'mammogram', 'management', 'marker', 'mass', 'medication', 'medications', 'migraine..."]}]},{"cell_type":"code","source":["data_list = [1, 45, 'M', 'White', 'Non-Hispanic', 'Asthma, Hypertension', '2023-01-05 to 2023-01-12', 'Blood Test: Normal, X-ray: Clear', 'MRI: No abnormalities', 'NonNasal swab: Positive for influenzae', 'Obesity, Smoking', 'Cough, Fever', 'Elevated heart rate', 'Antibiotics', 'Recovered','None']\n","columns = ['Patient ID','Age', 'Sex', 'Race', 'Ethnicity', 'Medical History', 'Hospitalization Data', 'Laboratory Data', 'Imaging Data', 'Microbiology Data', 'Risk Factors', 'Symptoms', 'Signs', 'Treatment', 'Outcomes','Unnamed: 16']\n","df_new = pd.DataFrame([data_list], columns=columns)\n","test_df = Processing(df_new)\n","X_data = test_df.process_data()\n","X_data.head()\n","\n","# Step 1: Get feature names used during training\n","feature_names_training = X_train.columns.tolist()\n","\n","# Step 2: Align feature names in test data\n","feature_names_test = X_data.columns.tolist()\n","\n","# Step 3: Identify unseen features in test data\n","unseen_features = set(feature_names_test) - set(feature_names_training)\n","\n","# Step 4: Drop unseen features from test data\n","X_data = X_data.drop(unseen_features, axis=1)\n","\n","# Step 5: Reorder features in test data to match the order in training data\n","X_data = X_data.reindex(columns=feature_names_training, fill_value=0)\n","\n","# # Step 6: Make predictions using the Random Forest classifier\n","# predict = rf.predict(X_data)\n","# print(predict)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Tj0aqEfktqV","executionInfo":{"status":"ok","timestamp":1684626664658,"user_tz":-330,"elapsed":601,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"93cb00e5-d617-4507-bba6-b5900d9e4e54"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["[-1.]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-a205f18bb11c>:38: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td[column] = td[column].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)) if isinstance(x, str) else x)\n","<ipython-input-5-a205f18bb11c>:43: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td[column] = td[column].apply(lambda x: ' '.join([word for word in str(x).split() if word.lower() not in stop_words]))\n","<ipython-input-5-a205f18bb11c>:73: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td.loc[:, column] = td[column].apply(lemmatize_column)\n","<ipython-input-5-a205f18bb11c>:79: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  td['Combined Text'] = td[text_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n"]}]},{"cell_type":"code","source":["# predict = grid_svc.predict(X_data)\n","predict = rf.predict(X_data)\n","print(predict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9RMCB2lauwC","executionInfo":{"status":"ok","timestamp":1684626678870,"user_tz":-330,"elapsed":438,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"93dc577f-2a28-429b-ead1-999d0b3d872f"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["[-1.]\n"]}]},{"cell_type":"code","source":["# Define a dictionary to map the numeric labels to their corresponding strings\n","label_mapping = {-1: 'None', 0: 'Other', 1: 'Pneumonia', 2: 'Influenza', 3: 'Unspecified Infection', 4: 'COVID-19', 5: 'Respiratory Infection'}\n","\n","# Map the predicted values to their corresponding strings\n","predict_strings = [label_mapping[label] for label in predict]\n","\n","# Print the predicted strings\n","for prediction in predict_strings:\n","    print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7y_oyIZk8B7","executionInfo":{"status":"ok","timestamp":1684626841105,"user_tz":-330,"elapsed":484,"user":{"displayName":"Naveen Kumar","userId":"17402594579712201923"}},"outputId":"af5d5cc2-85ea-430f-d379-6936eece086f"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]}]}